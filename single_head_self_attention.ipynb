{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "random.seed(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every word is split up into three vectors. Query vector, Key vector and a Value vector.\n",
    "They could really be the same in the beginning.\n",
    "\n",
    "Q = What am I looking for. \\\n",
    "    [sequence length x dk]\n",
    "\n",
    "K = What I can offer. \\\n",
    "    [sequence length x dk]\n",
    "\n",
    "V = What I actually offer. \\\n",
    "    [sequence length x dk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "L = Length of the input sequence. (Example use here: 'My name is X')\n",
    "q = query vector\n",
    "k = key vector\n",
    "v = value vector\n",
    "d_k, d_v = size of each of these vectors\n",
    "\"\"\"\n",
    "\n",
    "L, d_k, d_v = 4, 8, 8\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 1.26005904 -0.60960868  0.63954502  0.58826387 -0.41858783  0.44731905\n",
      "  -0.94809509 -2.26217274]\n",
      " [ 0.37111549 -0.74579406 -0.90044235  0.92477713 -0.54077297 -2.18289739\n",
      "  -1.528906    0.07248531]\n",
      " [ 0.73465036  1.02057437  0.10122983  0.37461743 -0.43145323  1.54817297\n",
      "  -0.7529665  -1.8003881 ]\n",
      " [ 0.55975041  0.09451513 -1.15170959  1.46535624  1.07301428 -0.03897071\n",
      "  -2.03871583 -0.11334228]]\n",
      "K\n",
      " [[-0.95171693 -0.71532861 -0.7193455   1.64216137  1.39948538  1.27760359\n",
      "  -0.99826132 -0.93016011]\n",
      " [-0.1093818  -1.87106191 -1.48165286 -0.45515292 -0.25495794  1.07766125\n",
      "   0.26877851 -1.33645628]\n",
      " [-0.53654945 -1.13072567 -0.75597795  0.28913273 -0.64078245  0.58431216\n",
      "   0.28864694 -0.55114978]\n",
      " [ 3.30912443  0.77069839  0.79505441 -1.93442701  0.06164565 -1.18686332\n",
      "  -0.6648031  -0.84443391]]\n",
      "V\n",
      " [[-0.09855224  0.99314645  1.75519943  0.55633223  1.04683343 -1.23577629\n",
      "  -0.96706165  0.42854073]\n",
      " [-1.44874153 -0.10017919  2.37106824  1.18087115 -0.4230308   0.37552648\n",
      "  -1.44814634 -1.41723147]\n",
      " [ 0.19840339 -1.11822734 -0.42476745 -2.06096971  0.47678579 -0.12082625\n",
      "  -1.41764738 -0.10445054]\n",
      " [ 0.95826576 -1.20267409  0.27240923 -0.90598858 -0.02629666  1.6055803\n",
      "   1.85063027 -1.25924192]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention\n",
    "\n",
    "$$\n",
    "\\text{Self-Attention} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}+M\\right) \\times V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( Q \\): Query matrix\n",
    "- \\( K \\): Key matrix\n",
    "- \\( V \\): Value matrix\n",
    "- \\( d_k \\): Dimensionality of keys (for scaling).\n",
    "- \\( M \\): Mask matrix\n",
    "\n",
    "\\\n",
    "In order to create an initial attention matrix, we need every single word to look at every single other word, just to see if it has a higher affinity towards it or not. \\\n",
    "This is represented by the query (for every word that I am looking for) and the key (what I currently have) \\\n",
    "$\\sqrt{d_k}$ is to minimize the variance and hence stabilize the values of $QK^T$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.77913976,  3.14470241,  1.20254957,  5.05422854],\n",
       "       [ 0.25979673, -0.45429855,  0.1820246 ,  1.66114748],\n",
       "       [ 2.9135832 ,  1.6717563 ,  0.43964674,  2.73023393],\n",
       "       [ 6.2269715 ,  0.08934572, -0.34919524, -0.26171583]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1020625626253824, 1.2402640337505473, 3.6893198395892632)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why we need sqrt(d_k) in denominator\n",
    "q.var(), k.var(), np.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1020625626253824, 1.2402640337505473, 0.4611649799486578)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98257428,  1.1118202 ,  0.42516548,  1.78693964],\n",
       "       [ 0.09185201, -0.16061879,  0.06435542,  0.58730432],\n",
       "       [ 1.03010722,  0.59105511,  0.15543859,  0.96528346],\n",
       "       [ 2.20156689,  0.03158848, -0.12345916, -0.09253052]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Masking\n",
    "\n",
    "* Required during decoder stage. Not required in the encoder stage.\n",
    "* This is to ensure words don't look at a future word when trying to generate current context of the current word.\n",
    "* Otherwise it will be cheating!. In reality you don't know the words that will be generated next so you can not create your vectors based off of those words.\n",
    "* During encoder stage - All inputs are passed simultaneously, therefore masking is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones( (L, L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask == 0] = -np.inf\n",
    "mask[mask == 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98257428,        -inf,        -inf,        -inf],\n",
       "       [ 0.09185201, -0.16061879,        -inf,        -inf],\n",
       "       [ 1.03010722,  0.59105511,  0.15543859,        -inf],\n",
       "       [ 2.20156689,  0.03158848, -0.12345916, -0.09253052]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "Converts a vector into a probability distribution. \\\n",
    "Values add up to 1. \\\n",
    "Interpretable and stable.\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "\n",
    "$\n",
    "x_i: \\text{Input value for the } i\\text{-th class.}\n",
    "$\n",
    "\n",
    "$\n",
    "n: \\text{Total number of classes.}\n",
    "$\n",
    "\n",
    "$\n",
    "e: \\text{Euler's number (approximately 2.718).}\n",
    "$\n",
    "\n",
    "$\n",
    "\\sum_{j=1}^n e^{x_j}: \\text{Sum of the exponential values for all classes, used for normalization.}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.56278456, 0.43721544, 0.        , 0.        ],\n",
       "       [0.485049  , 0.31268547, 0.20226552, 0.        ],\n",
       "       [0.7617229 , 0.08697358, 0.07448195, 0.07682157]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.56278456, 0.43721544, 0.        , 0.        ],\n",
       "       [0.485049  , 0.31268547, 0.20226552, 0.        ],\n",
       "       [0.7617229 , 0.08697358, 0.07448195, 0.07682157]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication of attention with value matrix.\n",
    "\n",
    "To better encapsulate the context of a word. \\\n",
    "Notice the difference between new_v and v below, for every subsequent row - showcasing better attention encapsulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09855224,  0.99314645,  1.75519943,  0.55633223,  1.04683343,\n",
       "        -1.23577629, -0.96706165,  0.42854073],\n",
       "       [-0.68887585,  0.5151276 ,  2.02446679,  0.82939029,  0.40418609,\n",
       "        -0.53128984, -1.17739931, -0.37845938],\n",
       "       [-0.46067293,  0.22422128,  1.50684052,  0.22222654,  0.47192725,\n",
       "        -0.50642937, -1.2086278 , -0.25641118],\n",
       "       [-0.11267879,  0.57211039,  1.5324853 ,  0.3033711 ,  0.79409628,\n",
       "        -0.7943144 , -0.8260043 ,  0.09865097]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09855224,  0.99314645,  1.75519943,  0.55633223,  1.04683343,\n",
       "        -1.23577629, -0.96706165,  0.42854073],\n",
       "       [-1.44874153, -0.10017919,  2.37106824,  1.18087115, -0.4230308 ,\n",
       "         0.37552648, -1.44814634, -1.41723147],\n",
       "       [ 0.19840339, -1.11822734, -0.42476745, -2.06096971,  0.47678579,\n",
       "        -0.12082625, -1.41764738, -0.10445054],\n",
       "       [ 0.95826576, -1.20267409,  0.27240923, -0.90598858, -0.02629666,\n",
       "         1.6055803 ,  1.85063027, -1.25924192]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "Converting all of the above into functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-1.71509064 -1.92415593 -1.27289131 -1.38139549  0.4973059  -0.22893632\n",
      "   1.89968557 -0.24670508]\n",
      " [-1.0403621  -0.93852842 -1.30753258 -0.75307571  1.02039821  0.82553694\n",
      "   0.8123335   0.24272788]\n",
      " [ 0.53678149  0.72682371 -0.57013903  1.43488115 -1.1783893   2.82176859\n",
      "   0.25996312 -1.37891998]\n",
      " [-1.13067757 -0.92246552  0.0159211  -0.56538126 -0.95904992 -0.16986172\n",
      "  -0.81385949 -0.35420748]]\n",
      "K\n",
      " [[ 0.38294428  1.2945802  -0.39063838 -0.05981553  1.16020338  1.20040557\n",
      "   0.70155104 -0.60762109]\n",
      " [-1.15423911 -0.35296605 -0.82701221  0.19377793 -0.88831067 -1.40537158\n",
      "  -0.0962384  -1.10318789]\n",
      " [-0.45286339  1.21208717  0.91555334 -0.61312538  0.26332605 -1.03603933\n",
      "  -0.59359496 -0.53816555]\n",
      " [ 1.38981481 -0.6328841  -1.0149947   0.86753006 -0.38059723  0.31700018\n",
      "   1.60578718 -0.06665863]]\n",
      "V\n",
      " [[ 0.12694417  0.52186389  0.96191713  1.36097137  0.06526419 -0.29196679\n",
      "  -1.01741379 -0.95722451]\n",
      " [ 0.48344737 -0.48286957 -0.31682075 -0.56666831 -1.18486621  0.45415092\n",
      "   0.53983956  0.30135372]\n",
      " [-1.27412034  0.72599276  0.82438819 -1.11934947  0.73199235 -0.05462457\n",
      "   0.68427828  0.26397771]\n",
      " [ 1.53941753  1.58604286 -1.33939112 -1.14963337 -1.62867512  0.30001968\n",
      "  -1.38044768 -0.76850049]]\n",
      "New V\n",
      " [[ 0.12694417  0.52186389  0.96191713  1.36097137  0.06526419 -0.29196679\n",
      "  -1.01741379 -0.95722451]\n",
      " [ 0.25945635  0.14840462  0.48661046  0.64446801 -0.39940908 -0.01463495\n",
      "  -0.43858297 -0.48941118]\n",
      " [ 0.08805402  0.42716905  0.81971965  1.02260043 -0.03003981 -0.20042625\n",
      "  -0.7605388  -0.75802547]\n",
      " [ 0.13902463  0.04584584 -0.05410734 -0.62130124 -0.69171002  0.27267233\n",
      "   0.30576697  0.11777927]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.62830016 0.37169984 0.         0.        ]\n",
      " [0.84010666 0.10533349 0.05455985 0.        ]\n",
      " [0.0651998  0.61329558 0.23482111 0.08668351]]\n"
     ]
    }
   ],
   "source": [
    "# For encoder, mask = None\n",
    "# For decoder, mask = mask\n",
    "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
