{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "random.seed(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every word is split up into three vectors. Query vector, Key vector and a Value vector.\n",
    "They could really be the same.\n",
    "\n",
    "Query = Length of input words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "L = Length of the input sequence. (Example use here: 'My name is X')\n",
    "q = query vector\n",
    "k = key vector\n",
    "v = value vector\n",
    "d_k, d_v = size of each of these vectors\n",
    "\"\"\"\n",
    "\n",
    "L, d_k, d_v = 4, 8, 8\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-1.41767666  0.96908187 -1.00570424 -1.38323781  1.13465927  0.79178207\n",
      "   0.53179063 -0.72936195]\n",
      " [-2.16976431  0.66542476 -0.99466221  0.60654108  1.71870799 -0.41256672\n",
      "  -1.79806925 -0.79852829]\n",
      " [ 0.89823134  0.55090139 -0.4092929   0.13166055  0.37747643  0.51839027\n",
      "  -1.41010075 -0.04922471]\n",
      " [ 0.24826843  0.5348298   0.75793618 -0.53615346 -0.19183252 -1.63892969\n",
      "   0.24689664  0.87612461]]\n",
      "K\n",
      " [[ 1.57491474 -0.86316812 -0.65884679 -0.37227769 -0.37581007  0.63731469\n",
      "   0.55250173 -0.23310638]\n",
      " [-1.63494969 -1.30428113 -0.80651226  0.47716264  1.13593996  0.41789594\n",
      "   0.41580284 -0.18122419]\n",
      " [ 2.06477795 -1.20794564 -0.94439751 -0.75339707 -0.47730805 -1.76071251\n",
      "  -0.50391215  0.54014202]\n",
      " [-0.55187274 -0.28083992  0.11199461  0.00648734 -1.78380066  0.48092649\n",
      "  -1.297721    0.73737651]]\n",
      "V\n",
      " [[-0.83391383 -1.53728569 -0.62599555 -0.87081994 -0.6608632  -1.30376564\n",
      "   0.25958102  1.95478765]\n",
      " [-1.69238889 -0.31480643  1.4093176   0.97424845  0.99885477 -0.55161911\n",
      "  -0.88639875  0.97593617]\n",
      " [ 0.5854254  -0.90880531 -0.88819252 -1.15829242 -0.13733918 -0.09430798\n",
      "   1.76882755 -1.03688959]\n",
      " [ 1.72302482 -0.18988972 -0.12444181 -1.03273095  1.45272653 -0.29987363\n",
      "  -0.70815872  0.18090176]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention\n",
    "\n",
    "$$\n",
    "\\text{Self-Attention} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}+M\\right) \\times V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( Q \\): Query matrix\n",
    "- \\( K \\): Key matrix\n",
    "- \\( V \\): Value matrix\n",
    "- \\( d_k \\): Dimensionality of keys (for scaling).\n",
    "- \\( M \\): Mask matrix\n",
    "\n",
    "\\\n",
    "In order to create an initial attention matrix, we need every single word to look at every single other word, just to see if it has a higher affinity towards it or not. \\\n",
    "This is represented by the query (for every word that I am looking for) and the key (what I currently have) \\\n",
    "$\\sqrt{d_k}$ is to minimize the variance and hence stabilize the values of $QK^T$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.34961473,  3.1780436 , -4.70349122, -2.48253399],\n",
       "       [-5.27817589,  4.94819001, -4.42067971, -0.61657652],\n",
       "       [ 0.58067407, -1.72614953,  1.06760165,  0.67417618],\n",
       "       [-1.41065307, -2.92951897,  2.88078195, -0.32619056]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9439222438403891, 0.8518506475999309, 7.96814250921233)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why we need sqrt(d_k) in denominator\n",
    "\n",
    "q.var(), k.var(), np.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9439222438403891, 0.8518506475999309, 0.9960178136515412)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47716086,  1.12360809, -1.66293527, -0.87770831],\n",
       "       [-1.86611698,  1.74944936, -1.5629463 , -0.21799272],\n",
       "       [ 0.20529928, -0.61028602,  0.37745418,  0.23835727],\n",
       "       [-0.49874117, -1.03574136,  1.01851023, -0.11532578]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Masking\n",
    "\n",
    "* Required during decoder stage. Not required in the encoder stage.\n",
    "* This is to ensure words don't look at a future word when trying to generate current context of the current word.\n",
    "* Otherwise it will be cheating!. In reality you don't know the words that will be generated next so you can not create your vectors based off of those words.\n",
    "* During encoder stage - All inputs are passed simultaneously, therefore masking is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones( (L, L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask == 0] = -np.inf\n",
    "mask[mask == 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47716086,        -inf,        -inf,        -inf],\n",
       "       [-1.86611698,  1.74944936,        -inf,        -inf],\n",
       "       [ 0.20529928, -0.61028602,  0.37745418,        -inf],\n",
       "       [-0.49874117, -1.03574136,  1.01851023, -0.11532578]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "Converts a vector into a probability distribution. \\\n",
    "Values add up to 1. \\\n",
    "Interpretable and stable.\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "\n",
    "$\n",
    "x_i: \\text{Input value for the } i\\text{-th class.}\n",
    "$\n",
    "\n",
    "$\n",
    "n: \\text{Total number of classes.}\n",
    "$\n",
    "\n",
    "$\n",
    "e: \\text{Euler's number (approximately 2.718).}\n",
    "$\n",
    "\n",
    "$\n",
    "\\sum_{j=1}^n e^{x_j}: \\text{Sum of the exponential values for all classes, used for normalization.}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.02619694, 0.97380306, 0.        , 0.        ],\n",
       "       [0.38019313, 0.16818996, 0.45161691, 0.        ],\n",
       "       [0.13138081, 0.07679195, 0.59905383, 0.19277341]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.02619694, 0.97380306, 0.        , 0.        ],\n",
       "       [0.38019313, 0.16818996, 0.45161691, 0.        ],\n",
       "       [0.13138081, 0.07679195, 0.59905383, 0.19277341]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication of attention with value matrix.\n",
    "\n",
    "To better encapsulate the context of a word. \\\n",
    "Notice how different new_v and v are every subsequent row - showcasing better attention encapsulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.83391383, -1.53728569, -0.62599555, -0.87081994, -0.6608632 ,\n",
       "        -1.30376564,  0.25958102,  1.95478765],\n",
       "       [-1.66989946, -0.34683165,  1.35599862,  0.9259133 ,  0.95537523,\n",
       "        -0.57132305, -0.85637759,  1.00157908],\n",
       "       [-0.33730312, -1.04784459, -0.40208889, -0.69032539, -0.145283  ,\n",
       "        -0.63105062,  0.74843997,  0.43906264],\n",
       "       [ 0.44333257, -0.80717343, -0.53008375, -0.93255716,  0.18765275,\n",
       "        -0.32795291,  0.88914442, -0.25451399]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.83391383, -1.53728569, -0.62599555, -0.87081994, -0.6608632 ,\n",
       "        -1.30376564,  0.25958102,  1.95478765],\n",
       "       [-1.69238889, -0.31480643,  1.4093176 ,  0.97424845,  0.99885477,\n",
       "        -0.55161911, -0.88639875,  0.97593617],\n",
       "       [ 0.5854254 , -0.90880531, -0.88819252, -1.15829242, -0.13733918,\n",
       "        -0.09430798,  1.76882755, -1.03688959],\n",
       "       [ 1.72302482, -0.18988972, -0.12444181, -1.03273095,  1.45272653,\n",
       "        -0.29987363, -0.70815872,  0.18090176]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "Converting all of the above into functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-1.41767666  0.96908187 -1.00570424 -1.38323781  1.13465927  0.79178207\n",
      "   0.53179063 -0.72936195]\n",
      " [-2.16976431  0.66542476 -0.99466221  0.60654108  1.71870799 -0.41256672\n",
      "  -1.79806925 -0.79852829]\n",
      " [ 0.89823134  0.55090139 -0.4092929   0.13166055  0.37747643  0.51839027\n",
      "  -1.41010075 -0.04922471]\n",
      " [ 0.24826843  0.5348298   0.75793618 -0.53615346 -0.19183252 -1.63892969\n",
      "   0.24689664  0.87612461]]\n",
      "K\n",
      " [[ 1.57491474 -0.86316812 -0.65884679 -0.37227769 -0.37581007  0.63731469\n",
      "   0.55250173 -0.23310638]\n",
      " [-1.63494969 -1.30428113 -0.80651226  0.47716264  1.13593996  0.41789594\n",
      "   0.41580284 -0.18122419]\n",
      " [ 2.06477795 -1.20794564 -0.94439751 -0.75339707 -0.47730805 -1.76071251\n",
      "  -0.50391215  0.54014202]\n",
      " [-0.55187274 -0.28083992  0.11199461  0.00648734 -1.78380066  0.48092649\n",
      "  -1.297721    0.73737651]]\n",
      "V\n",
      " [[-0.83391383 -1.53728569 -0.62599555 -0.87081994 -0.6608632  -1.30376564\n",
      "   0.25958102  1.95478765]\n",
      " [-1.69238889 -0.31480643  1.4093176   0.97424845  0.99885477 -0.55161911\n",
      "  -0.88639875  0.97593617]\n",
      " [ 0.5854254  -0.90880531 -0.88819252 -1.15829242 -0.13733918 -0.09430798\n",
      "   1.76882755 -1.03688959]\n",
      " [ 1.72302482 -0.18988972 -0.12444181 -1.03273095  1.45272653 -0.29987363\n",
      "  -0.70815872  0.18090176]]\n",
      "New V\n",
      " [[-0.83391383 -1.53728569 -0.62599555 -0.87081994 -0.6608632  -1.30376564\n",
      "   0.25958102  1.95478765]\n",
      " [-1.66989946 -0.34683165  1.35599862  0.9259133   0.95537523 -0.57132305\n",
      "  -0.85637759  1.00157908]\n",
      " [-0.33730312 -1.04784459 -0.40208889 -0.69032539 -0.145283   -0.63105062\n",
      "   0.74843997  0.43906264]\n",
      " [ 0.44333257 -0.80717343 -0.53008375 -0.93255716  0.18765275 -0.32795291\n",
      "   0.88914442 -0.25451399]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.02619694 0.97380306 0.         0.        ]\n",
      " [0.38019313 0.16818996 0.45161691 0.        ]\n",
      " [0.13138081 0.07679195 0.59905383 0.19277341]]\n"
     ]
    }
   ],
   "source": [
    "# For encoder, mask = None\n",
    "# For decoder, mask = mask\n",
    "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
